{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Count Vector Config Overview\n",
    "1. define stopwords\n",
    "2. define extent of lemmatization\n",
    "3. define tokenizer function (wrapper function for stopwords, lemmatization)\n",
    "4. define vector config (wrapper object for tokenizer function)\n",
    "5. define vectorization and matrix transformation\n",
    "\n",
    "Vector Config ToDo\n",
    "Apply more regorous corpus preprocessing:\n",
    "=> remove all non-english entries, experiement with ngram forms \n",
    "=> remove redundant tokens, limit vocabulary? \n",
    "'''\n",
    "\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize constants\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def define_sw():\n",
    "    custom_stop_words = ['–', '\\u2019', 'u', '\\u201d', '\\u201d.',\n",
    "                         '\\u201c', 'say', 'saying', 'sayings',\n",
    "                         'says', 'us', 'un', '.\\\"', 'would',\n",
    "                         'let', '.”', 'said', ',”', 'ax', 'max',\n",
    "                         'b8f', 'g8v', 'a86', 'pl', '145', 'ld9', '0t',\n",
    "                         '34u']\n",
    "    return set(stopwords.words('english') + custom_stop_words)\n",
    "\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    tag = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    tokens = []\n",
    "    sw = define_sw()\n",
    "    punct = set(punctuation)\n",
    "\n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # tokenize each sentence\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in sw:\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "\n",
    "            # Append lemmatized token to list\n",
    "            tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def generate_vector(params):\n",
    "    return CountVectorizer(tokenizer=cab_tokenizer, ngram_range=tuple(params['ngram_range']),\n",
    "                           min_df=params['min_doc_frequency'], max_df=params['max_doc_frequency'])\n",
    "\n",
    "\n",
    "def vectorize(tf_vectorizer, df):\n",
    "    # fit count vectorizer to supplied corpus, return term frequency matrix\n",
    "    df = df.reindex(columns=['tweet'])  # reindex on tweet\n",
    "\n",
    "    tf_matrix = tf_vectorizer.fit_transform(df['tweet'])\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    return tf_matrix, tf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Dynamic Feature Generation Overview\n",
    "1. create LDA model object based upon a supplied term frequency corpus (vectorized corpus)\n",
    "2. generate Document/Topic and Topic/Word (unused currently) distributions based upon LDA model object\n",
    "3. calculate DT distribution entropy for all entries within term frequency matrix\n",
    "4. calculate LOSS and GOSS scores for all entries within term frequency matrix, utilizing DT distribution\n",
    "\n",
    "Dynamic Feature ToDo\n",
    "Optimize GOSS/LOSS functions\n",
    "'''\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import scipy as scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import sys\n",
    "import util\n",
    "\n",
    "def create_lda(tf_matrix, params):\n",
    "    return LatentDirichletAllocation(n_components=params['lda_topics'], max_iter=params['iterations'],\n",
    "                                     learning_method='online', learning_offset=10,\n",
    "                                     random_state=0).fit(tf_matrix)\n",
    "\n",
    "\n",
    "def create_tw_dist(model):\n",
    "    # return normalized topic-word distribution\n",
    "    normTWDist = model.components_ / \\\n",
    "        model.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    return normTWDist\n",
    "\n",
    "\n",
    "def create_dt_dist(model, tf_matrix):\n",
    "    # return normalized document-topic distribution\n",
    "    normDTDist = model.transform(tf_matrix)\n",
    "\n",
    "    return normDTDist\n",
    "\n",
    "\n",
    "def entropy_single(x):\n",
    "    # calculate entropy for a given sequence of values\n",
    "    return scipy.stats.entropy(x)\n",
    "\n",
    "\n",
    "def entropy_all(dt_dist):\n",
    "    # calculate entropy for an entire document-topic distribution\n",
    "    np_entropy = np.apply_along_axis(entropy_single, axis=1, arr=dt_dist)\n",
    "\n",
    "    return pd.DataFrame(np_entropy, columns=['dt_entropy'])\n",
    "\n",
    "\n",
    "def single_goss(topic_dist, i, k):\n",
    "    # calculate GOSS score for a single particular user/topic (i/k) combination\n",
    "\n",
    "    # 1.0 return mu(xk) for specific topic, sum topic probabilities for all\n",
    "    # users, average across all users\n",
    "    mu_xk = np.sum(topic_dist[:, k]) / topic_dist.shape[0]\n",
    "\n",
    "    # 2.0 GOSS equation numerator\n",
    "    goss_numerator = topic_dist[i, k] - mu_xk\n",
    "\n",
    "    # 3.0 for all users specific topic probability:\n",
    "    # - sum the squared difference of their relevant topic probability\n",
    "    # - find the square of this sum\n",
    "    goss_denominator = 0\n",
    "    for user_prob in topic_dist[:, k]:\n",
    "        goss_denominator += (user_prob - mu_xk) ** 2\n",
    "\n",
    "    # 3.1 find sqrt of goss_denominator\n",
    "    goss_denominator = sqrt(goss_denominator)\n",
    "\n",
    "    # 4.0 divide numerator/denominator to find final GOSS score for user/topic\n",
    "    # combination\n",
    "    return goss_numerator / goss_denominator\n",
    "\n",
    "\n",
    "def all_goss(topic_dist):\n",
    "    # calculate GOSS scores for a particular topic distribution\n",
    "    goss = []\n",
    "    topics = range(topic_dist.shape[1])\n",
    "    topic_labels = list('goss_' + str(each) for each in topics)\n",
    "\n",
    "    for user in range(topic_dist.shape[0]):  # each user\n",
    "        temp_goss = list(single_goss(topic_dist, user, topic)\n",
    "                         for topic in topics)  # calculate all GOSS scores per topic\n",
    "        goss.append(temp_goss)  # store all GOSS via nested lists\n",
    "\n",
    "    np_goss = np.array(goss)  # recast as np array..\n",
    "    return pd.DataFrame(goss, columns=topic_labels)  # and then to pandas df..\n",
    "\n",
    "\n",
    "def single_loss(topic_dist, i, k):\n",
    "    # calculate loss score for a particular user/topic (i/k) combination\n",
    "    # 1.0 return mu(xi) for specific user, sum topic probabilities, return\n",
    "    # average\n",
    "    mu_xi = np.sum(topic_dist[i, :]) / topic_dist.shape[1]\n",
    "\n",
    "    # 2.0 calculate muXI diff - GOSS equation numerator\n",
    "    loss_numerator = topic_dist[i, k] - mu_xi\n",
    "\n",
    "    # 3.0 for all topics (k) and a specific user (i):\n",
    "    # - sum the squared difference of all associated topic probabilities and mu(xi)\n",
    "    # - find the square of this sum\n",
    "    loss_denominator = 0\n",
    "    for user_prob in topic_dist[i, :]:\n",
    "        loss_denominator += (user_prob - mu_xi) ** 2\n",
    "\n",
    "    # 3.1 find sqrt of loss denominator\n",
    "    loss_denominator = sqrt(loss_denominator)\n",
    "\n",
    "    # 4.0 divide loss numerator by loss denominator to find loss score for\n",
    "    # specific user\n",
    "    return loss_numerator / loss_denominator\n",
    "\n",
    "\n",
    "def all_loss(topic_dist):\n",
    "    # calculate LOSS scores for a particular topic distribution\n",
    "    loss = []\n",
    "    topics = range(topic_dist.shape[1])\n",
    "    topic_labels = list('loss_' + str(each) for each in topics)\n",
    "\n",
    "    for user in range(topic_dist.shape[0]):  # each user\n",
    "        temp_loss = list(single_loss(topic_dist, user, topic)\n",
    "                         for topic in topics)  # calculate all loss scores per topic\n",
    "        # store all loss scores for each user via nested lists\n",
    "        loss.append(temp_loss)\n",
    "\n",
    "    np_loss = np.array(loss)  # cast to np array..\n",
    "    # and finally to pandas df..\n",
    "    return pd.DataFrame(np_loss, columns=topic_labels)\n",
    "\n",
    "\n",
    "def generate_dynamic_features(tf_matrix, params):\n",
    "    # 1. fit LDA model using term frequency matrix\n",
    "    lda = create_lda(tf_matrix, params)\n",
    "\n",
    "    # 2. generate document-topic distribution\n",
    "    dt_dist = create_dt_dist(lda, tf_matrix)\n",
    "\n",
    "    # 3. retrieve entropy for document-topic distribution\n",
    "    dt_entropy = entropy_all(dt_dist)\n",
    "\n",
    "    # 4. retrieve GOSS and LOSS scores\n",
    "    goss_df = all_goss(dt_dist)\n",
    "    loss_df = all_loss(dt_dist)\n",
    "\n",
    "    # 5. glue new features together into single df\n",
    "    dynamic_features = pd.concat([dt_entropy, goss_df, loss_df], axis=1)\n",
    "\n",
    "    # sanitize for possible NAN entries, possible bug withing GOSS/LOSS\n",
    "    # generator functions\n",
    "    dynamic_features = dynamic_features.dropna()\n",
    "\n",
    "    return dynamic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating dynamic features for honeypot dataset..\n",
      "\n",
      "Executing Feature Engineering with the following params:\n",
      " {\n",
      "    \"count_vector\": {\n",
      "        \"max_doc_frequency\": 0.85,\n",
      "        \"min_doc_frequency\": 0.15,\n",
      "        \"ngram_range\": [\n",
      "            1,\n",
      "            2\n",
      "        ]\n",
      "    },\n",
      "    \"lda_modelling\": {\n",
      "        \"iterations\": 5,\n",
      "        \"lda_topics\": 5\n",
      "    }\n",
      "}\n",
      "\n",
      "Dynamic feature generation completed in 24.76258158683777 seconds. Features saved to:\n",
      " ../../../data_sets/honey_pot/final_features/dynamic_features.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Feature Engineering Overview\n",
    "1. import intermediate, dynamic features dataframe (tweet corpus)\n",
    "2. configure and generate count vector\n",
    "3. create term frequency matrix by vectorizing tweet corpus, applying lemmatization, stemming etc.\n",
    "4. generate dynamic features using term frequency matrix (GOSS, LOSS, document-topic distribution entropy)\n",
    "5. export dynamic features dataframe\n",
    "'''\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import util\n",
    "\n",
    "# define import/export directories\n",
    "dirs = {'dynamic_import': '../../../data_sets/honey_pot/preprocessed/dynamic_features_intermediate.csv',\n",
    "        'dynamic_export': '../../../data_sets/honey_pot/final_features/dynamic_features.csv',\n",
    "        'param_import': './configs/hp_fe_config.json'}\n",
    "\n",
    "\n",
    "def main():\n",
    "    st = time.time()\n",
    "    print('\\nGenerating dynamic features for honeypot dataset..\\n')\n",
    "    params = util.parse_params(dirs['param_import'], 'Feature Engineering')\n",
    "\n",
    "    # 1.import dataframe\n",
    "    df = util.import_frame(dirs['dynamic_import'])\n",
    "\n",
    "    # 2.configure count vector\n",
    "    cv = generate_vector(params['count_vector'])\n",
    "\n",
    "    # 3. vectorize corpus, create term frequency matrix\n",
    "    tf_matrix, tf_feature_names = vectorize(cv, df)\n",
    "\n",
    "    # 4.generate dynamic features\n",
    "    df = generate_dynamic_features(tf_matrix, params['lda_modelling'])\n",
    "\n",
    "    # 5.export dataframe\n",
    "    util.export_frame(df, dirs['dynamic_export'])\n",
    "\n",
    "    et = time.time() - st\n",
    "    print('\\nDynamic feature generation completed in {0} seconds. Features saved to:\\n {1}'.format(\n",
    "        et, dirs['dynamic_export']))\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

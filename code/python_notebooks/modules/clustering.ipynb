{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing cluster filtering for honeypot dataset..\n",
      "Executing Clustering with the following params:\n",
      " {\n",
      "    \"cluster_config\": {\n",
      "        \"clusters\": 3,\n",
      "        \"max_iterations\": 300\n",
      "    },\n",
      "    \"clustering_features\": {\n",
      "        \"about_me_length\": false,\n",
      "        \"frequent_words\": false,\n",
      "        \"num_annotations\": false,\n",
      "        \"num_followers\": false,\n",
      "        \"num_followings\": false,\n",
      "        \"num_http\": false,\n",
      "        \"num_tweets\": false,\n",
      "        \"num_unique_words\": true,\n",
      "        \"td_entropy\": true,\n",
      "        \"tweet_avg_length\": false,\n",
      "        \"user_name_length\": false\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"cluster0\": {\n",
      "        \"composition\": {\n",
      "            \"0\": 24,\n",
      "            \"1\": 9\n",
      "        },\n",
      "        \"size\": 33\n",
      "    },\n",
      "    \"cluster1\": {\n",
      "        \"composition\": {\n",
      "            \"0\": 25,\n",
      "            \"1\": 10\n",
      "        },\n",
      "        \"size\": 35\n",
      "    },\n",
      "    \"cluster2\": {\n",
      "        \"composition\": {\n",
      "            \"0\": 17,\n",
      "            \"1\": 15\n",
      "        },\n",
      "        \"size\": 32\n",
      "    },\n",
      "    \"total_entries\": 100\n",
      "}\n",
      "\n",
      "Cluster filtering completed in 0.05133962631225586 seconds. Individual frames saved to:\n",
      "\n",
      " ../../../data_sets/honey_pot/final_features/cluster_frames\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preliminary Clustering Overview\n",
    "1. create master feature dataframe (merge static/dynamic), selectively create cluster features dataframe\n",
    "2. perform kmeans clustering using cluster features dataframe, append cluster allocation as additional feature within master dataframe\n",
    "3. segment master dataframe based upon newly created cluster allocation, generate n cluster dataframes\n",
    "4. evaluate the composition of the newly created, segmented dataframes\n",
    "5. export cluster dataframes and cluster analysis\n",
    "\n",
    "Cluster ToDo\n",
    "Implement cluster analysis, export as seperate data object\n",
    "Implement a variety of cluster algorithms => implement mechanism that is capable of selecting the \"best\" clusters\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import util\n",
    "\n",
    "\n",
    "# define IO directories and files\n",
    "dirs = {'static_features': '../../../data_sets/honey_pot/final_features/static_features.csv',\n",
    "        'dynamic_features': '../../../data_sets/honey_pot/final_features/dynamic_features.csv',\n",
    "        'cluster_frames': '../../../data_sets/honey_pot/final_features/cluster_frames',\n",
    "        'param_import': './configs/hp_cluster_config.json',\n",
    "       'report_output': '../../output/clustering_results.json'}\n",
    "\n",
    "\n",
    "def consolidate_features(static_features, dynamic_features):\n",
    "    # join static and dynamic features\n",
    "    static = util.import_frame(static_features)\n",
    "    dynamic = util.import_frame(dynamic_features)\n",
    "\n",
    "    # join features along axis\n",
    "    master_df = pd.concat([static, dynamic], axis=1)\n",
    "\n",
    "    # replace infinite entries with nan\n",
    "    master_df.replace([np.inf, -np.inf], np.nan)\n",
    "    master_df = master_df.dropna()  # drop all nan entries\n",
    "\n",
    "    return master_df\n",
    "\n",
    "\n",
    "def scale_features(df):\n",
    "    # scale features to ensure clustering is not skewed by large values\n",
    "    scaler = StandardScaler()\n",
    "    df = df.as_matrix()\n",
    "\n",
    "    return scaler.fit_transform(df)\n",
    "\n",
    "\n",
    "def configure_df(static, dynamic, params):\n",
    "    # consolidate static/dynamic features, select features, scale values\n",
    "    master_df = consolidate_features(static, dynamic)\n",
    "\n",
    "    # configure clustering dataframe\n",
    "    features = util.extract_features(params['clustering_features'])\n",
    "    cluster_df = util.choose_features(\n",
    "        master_df, ['dt_entropy', 'num_unique_words'])\n",
    "\n",
    "    # return master feature df and scaled cluster df\n",
    "    return master_df, scale_features(cluster_df)\n",
    "\n",
    "\n",
    "def create_kmeans(df, df_matrix, params):\n",
    "    km = KMeans(n_clusters=params['cluster_config']['clusters'], random_state=42,\n",
    "                max_iter=params['cluster_config']['max_iterations']).fit(df_matrix)  # create/fit kmeans to matrix\n",
    "\n",
    "    df['cluster'] = km.labels_  # augment cluster result as attribute\n",
    "\n",
    "    return df, km\n",
    "\n",
    "def evaluate_frames(master_df, seg_df, results):\n",
    "    for idx, each in enumerate(seg_df): #membership composition\n",
    "        composition = each['user_type'].value_counts().tolist()\n",
    "        inter = {'size':each.shape[0], 'composition':{'0':composition[0], '1':composition[1]}}\n",
    "        results['cluster'+str(idx)] = inter #append intermediate results to main\n",
    "    \n",
    "    results['total_entries'] = master_df.shape[0]\n",
    "    print(json.dumps(results, sort_keys=True, indent=4))\n",
    "    return results\n",
    "\n",
    "def segment_df(df, params):\n",
    "    # filter and split main df based upon cluster allocation\n",
    "    segmented_frames = []\n",
    "\n",
    "    # segment df based upon cluster\n",
    "    for cluster in range(params['cluster_config']['clusters']):\n",
    "        segmented_frames.append(df.loc[df['cluster'] == cluster])\n",
    "\n",
    "    return segmented_frames\n",
    "\n",
    "\n",
    "def main():\n",
    "    st = time.time()\n",
    "    print('\\nPerforming cluster filtering for honeypot dataset..')\n",
    "    params = util.parse_params(dirs['param_import'], 'Clustering')\n",
    "    results = dict()\n",
    "\n",
    "    # 1.create master dataframe\n",
    "    df, cluster_matrix = configure_df(dirs['static_features'], dirs[\n",
    "                                      'dynamic_features'], params)\n",
    "\n",
    "    # 2.perform kmeans clustering, append cluster allocation as feature\n",
    "    df, km = create_kmeans(df, cluster_matrix, params)\n",
    "\n",
    "    # 3.segment master dataframe based upon cluster allocation\n",
    "    seg_df = segment_df(df, params)\n",
    "    \n",
    "    # 3.describe cluster segment composition\n",
    "    results = evaluate_frames(df, seg_df, results)\n",
    "\n",
    "    # 5.export segmented dataframes and clustering results\n",
    "    util.export_frames_destructive(seg_df, dirs['cluster_frames'])\n",
    "    util.export_results(dirs['report_output'], results)\n",
    "\n",
    "    et = time.time() - st\n",
    "    print('\\nCluster filtering completed in {0} seconds. Individual frames saved to:\\n\\n {1}'.format(\n",
    "        et, dirs['cluster_frames']))\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

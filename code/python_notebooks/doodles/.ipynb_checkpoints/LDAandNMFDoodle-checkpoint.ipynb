{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                       0\n",
       "0     com\\n\\n\\n\\nOn a DA revolver, you get another t...\n",
       "1     \\nI don't, though when I was in Israel I did m...\n",
       "2     **********************************************...\n",
       "3     What happened in Waco is not the fault of the ...\n",
       "4     To my fellow Columbian, I must ask, why do you...\n",
       "5     \\n#Rick Anderson replied to my letter with...\\...\n",
       "6     \\nSome people pay shares that are more \"fair\" ...\n",
       "7     Hey Serdar,\\n           What nationality are y...\n",
       "8     \\n\\n\\n\\nBecause there are about 40 homicides t...\n",
       "9     \\nRight now, I'm just going to address this po...\n",
       "10    \\n\\nFreedom of speech does not mean that other...\n",
       "11    \\n\\n\\n\\n\\nYes, I am pro-gun, and yes, I do dis...\n",
       "12    \\nThe letter implies that both warrants were i...\n",
       "13    \\nAviation Week March 15 1993 p.48\\n\\n\"the CBO...\n",
       "14    \\nExcellently put!\\n\\nEven as a libertarian, I...\n",
       "15    04/19/1993 0000  Lezghis Astir\\n\\nBy NEJLA SAM...\n",
       "16    \\n\\n\\nAs I recall, in the 60's the Kennedy Adm...\n",
       "17    \\nSo it was a complete non-sequitur, is that i...\n",
       "18    RE: Red, wwhite, and black, the colors of the ...\n",
       "19    THE WHITE HOUSE\\n\\n                    Office ...\n",
       "20    \\n# Well said Mr. Beyer :)\\n\\nHe-he. The great...\n",
       "21    # |## |#2. Professors get summers off; industr...\n",
       "22    I replied to your message, however, it is list...\n",
       "23    It's all my fault. \\nI am in violation of one ...\n",
       "24    # #The article also contains numbers on the nu...\n",
       "25    \\nI'm not aware that the US government conside...\n",
       "26    \\n\\nWhy don't you call the City and ask? Oak P...\n",
       "27    \\nConsidering that Clinton received a draft no...\n",
       "28    Avi,\\n   For your information, Islam permits f...\n",
       "29    This post has all the earmarks of a form progr...\n",
       "...                                                 ...\n",
       "1922  \\n\\nCNN just claimed he bought 104 \"semi-autom...\n",
       "1923  \\nCould it be because you're British, Phill, a...\n",
       "1924  \\nLet's see if I have this straight.  A law is...\n",
       "1925  # # Unfortunately, homosexuals don't believe i...\n",
       "1926      >>>Does the greatly increased rates of inc...\n",
       "1927  \\nHmm.  I beg to differ.  It will probably mak...\n",
       "1928                                                 \\n\n",
       "1929                                             ^^^...\n",
       "1930  \\ns,\\n\\nThis country is hardly ruined. In fact...\n",
       "1931  :You are loosing.\\n\\n\"Loosing\"?  Well, I'll av...\n",
       "1932  \\n\\nWell, these are Armenian and Jewish schola...\n",
       "1933  THE WHITE HOUSE\\n\\n                    Office ...\n",
       "1934  \\tI have just started reading the articles in ...\n",
       "1935  \\nJesus certainly demonstrated the great depth...\n",
       "1936  Oops, I forgot to set read permission.  It's f...\n",
       "1937  [ ... ]\\n\\nThen it also supports basing such r...\n",
       "1938  \\nYes, I saw today in 6 o'clock news on KCBS h...\n",
       "1939  }     A note on the lighter side, I've noticed...\n",
       "1940  \\nDoes this organization have an official e-ma...\n",
       "1941                                                   \n",
       "1942  ri\\n\\n\\n\\nI do agree with you, in a way.  The ...\n",
       "1943  \\nhow so? i think you're making assumptions he...\n",
       "1944  \\nYep, that's pretty much it. I'm not a Jew bu...\n",
       "1945  \\n\\n\\n\\n\\nOf course.  The term must be rigidly...\n",
       "1946  \\nI don't know where YOU live, but this is not...\n",
       "1947  I am glad that you recognize that people shoul...\n",
       "1948  \\nAnd another survivor claims he heard someone...\n",
       "1949  Center for Policy Research writes...\\n\\n\\n\\n  ...\n",
       "1950  Actually not Jim.  I just said that everyone e...\n",
       "1951  \\n\\nYou are correct. See today's (4/21) Washin...\n",
       "\n",
       "[1952 rows x 1 columns]>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "#limit retrieval\n",
    "categories = ['talk.politics.guns','talk.politics.mideast','talk.politics.misc','talk.religion.misc']\n",
    "\n",
    "#reformat to dataframe\n",
    "dataset = fetch_20newsgroups(shuffle=True, categories=categories, random_state=1, remove=('headers','footers','quotes'))\n",
    "df = pd.DataFrame(dataset.data)\n",
    "\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemming, Lemmatization and Stopword processing\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "\n",
    "# initialize constants, lematizer, punctuation and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "#define stopwords\n",
    "custom_stop_words = ['–', '\\u2019', 'u', '\\u201d', '\\u201d.',\n",
    "                     '\\u201c', 'say', 'saying', 'sayings',\n",
    "                     'says', 'us', 'un', '.\\\"', 'would',\n",
    "                     'let', '.”', 'said', ',”', 'ax','max',\n",
    "                     'b8f','g8v','a86','pl','145','ld9','0t',\n",
    "                     '34u']\n",
    "                     \n",
    "stopwords = set(sw.words('english') + custom_stop_words)\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    # collapse word inflections into single representation\n",
    "    tag = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    # tokenize the corpus\n",
    "    tokens = []\n",
    "\n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # tokenize each sentence\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "\n",
    "            # Append lemmatized token to list\n",
    "            tokens.append(lemma)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized corpus\n",
      "CPU times: user 1min 31s, sys: 1.12 s, total: 1min 33s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Preprocessing and Vector Fitting\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#NMF requires TFIDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=cab_tokenizer,ngram_range=(1,2),\n",
    "                                   min_df=0.1, max_df=0.90)\n",
    "tfidf = tfidf_vectorizer.fit_transform(df[0])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "#LDA requires Count Vectorizer\n",
    "tf_vectorizer = CountVectorizer(tokenizer=cab_tokenizer,ngram_range=(1,2),\n",
    "                                   min_df=0.1, max_df=0.90)\n",
    "tf = tf_vectorizer.fit_transform(df[0])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Vectorized corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models fitted\n",
      "CPU times: user 4.71 s, sys: 12 ms, total: 4.72 s\n",
      "Wall time: 3.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Model Generation\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "topics = 10\n",
    "\n",
    "#Non-Negative Matrix Factorization - fit model using tfidf vector\n",
    "nmf = NMF(n_components=topics,random_state=1,alpha=0.1,l1_ratio=0.5,init='nndsvd').fit(tfidf)\n",
    "\n",
    "#Latent Dirilicht Analysis - fit the model using term frequency vector\n",
    "lda = LatentDirichletAllocation(n_components=topics,max_iter=5,learning_method='online',learning_offset=50,random_state=0).fit(tf)\n",
    "\n",
    "print(\"Models fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is a test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is another test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               document\n",
       "0        this is a test\n",
       "1  this is another test"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {'document': ['this is a test',\n",
    "         'this is another test']}\n",
    "\n",
    "df = pd.DataFrame(data=corpus)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1952, 82)\n",
      "(1952, 10)\n",
      "(1952,)\n"
     ]
    }
   ],
   "source": [
    "#10 topics, each comprised of 82 wordsprint, probabilties not normalized however\n",
    "#1952 articles\n",
    "# print(lda.components_.shape)\n",
    "\n",
    "# print(lda.shape)\n",
    "# print(type(lda))\n",
    "\n",
    "print(tf.shape)\n",
    "\n",
    "test = lda.transform(tf)\n",
    "print(test.shape)\n",
    "print(test[:,0].shape)\n",
    "print(test[0])\n",
    "print(sum(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF Topics:\n",
      "Topic 0:\n",
      "get go see time well take use good give want\n",
      "Topic 1:\n",
      "gun use get law number like year problem time point\n",
      "Topic 2:\n",
      "state law right use case may 1 also since force\n",
      "Topic 3:\n",
      "people many kill first force like tell problem live country\n",
      "Topic 4:\n",
      "one kill child two another many consider come number seem\n",
      "Topic 5:\n",
      "post point get could number part question want new good\n",
      "Topic 6:\n",
      "make see much child look life case well problem start\n",
      "Topic 7:\n",
      "think like case really might want take question see something\n",
      "Topic 8:\n",
      "know like something tell even go believe look thing come\n",
      "Topic 9:\n",
      "government need case 2 time right year also force fact\n",
      "\n",
      "LDA Topics:\n",
      "Topic 0:\n",
      "state people post government make want right time get ask\n",
      "Topic 1:\n",
      "1 2 kill people state year one government number two\n",
      "Topic 2:\n",
      "think go make work know get want well look time\n",
      "Topic 3:\n",
      "people right like government good life think even case much\n",
      "Topic 4:\n",
      "child show good like really make question without last first\n",
      "Topic 5:\n",
      "gun use one get problem time make year number many\n",
      "Topic 6:\n",
      "go one know come people take tell get see think\n",
      "Topic 7:\n",
      "use one point mean give believe make claim question since\n",
      "Topic 8:\n",
      "law may fact could see question use since make case\n",
      "Topic 9:\n",
      "one many world new year also take first time people\n"
     ]
    }
   ],
   "source": [
    "#display results\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in (-topic).argsort()[:no_top_words]]))\n",
    "        \n",
    "no_top_words = 10\n",
    "\n",
    "print(\"NMF Topics:\")\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print(\"\\nLDA Topics:\")\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

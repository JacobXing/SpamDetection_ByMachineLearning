{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samhardyhey/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py:1215: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code, glob, local_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish restructuring and concatenating tweet sets\n",
      "CPU times: user 22.4 s, sys: 1.13 s, total: 23.5 s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#user tweet restructuring and concatenation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', 40) #display entirety of tweet document\n",
    "\n",
    "#read in single user tweet set - illegitimate or legitimate\n",
    "def readIn(userTweets):\n",
    "    tweets = pd.read_csv(userTweets)\n",
    "    \n",
    "    #variable removal\n",
    "    tweets = tweets.drop(['TweetID','CreatedAt'], axis=1) #strip unnecessary variables\n",
    "    tweets['UserID'] = pd.to_numeric(tweets['UserID'], errors='coerce') #flag NAN entries in UserID column\n",
    "    tweets = tweets.dropna(subset=['UserID']) #remove row entries with malformed UserID\n",
    "\n",
    "    #tweet collating\n",
    "    tweets = tweets.groupby(['UserID'])['Tweet'].apply(list) #groupby userID, pool tweets into single list\n",
    "    tweetJoin = lambda x: ' '.join(x) #join tweets together into single document\n",
    "    tweets = tweets.apply(tweetJoin)\n",
    "\n",
    "    #dataframe reformatting\n",
    "    tweets = tweets.to_frame() #cast back to frame\n",
    "    tweets = tweets.reset_index() #reset/adjust index\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "#read in both tweet sets, concat into single df\n",
    "def readAll():\n",
    "    cp_tweets = readIn('../../data-sets/honey-pot/raw/content_polluters_tweets.csv')\n",
    "    lu_tweets = readIn('../../data-sets/honey-pot/raw/legitimate_users_tweets.csv')\n",
    "    \n",
    "    #flag as illegitimate/legitimate users\n",
    "    cp_tweets['UserType'] = 1\n",
    "    lu_tweets['UserType'] = 0\n",
    "    \n",
    "    #merge data frames\n",
    "    return pd.concat([cp_tweets, lu_tweets])\n",
    "\n",
    "allTweets = readAll()\n",
    "print(\"Finish restructuring and concatenating tweet sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Stemming, Lemmatization, Tokenization Utilty Functions\n",
      "CPU times: user 192 ms, sys: 121 ms, total: 313 ms\n",
      "Wall time: 523 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Stopword generation, Lemmatization and Tokenizer object generation\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "\n",
    "# initialize constants, lematizer, punctuation and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# define/return stopwords\n",
    "def define_sw():\n",
    "    custom_stop_words = ['–', '\\u2019', 'u', '\\u201d', '\\u201d.',\n",
    "                         '\\u201c', 'say', 'saying', 'sayings',\n",
    "                         'says', 'us', 'un', '.\\\"', 'would',\n",
    "                         'let', '.”', 'said', ',”', 'ax','max',\n",
    "                         'b8f','g8v','a86','pl','145','ld9','0t',\n",
    "                         '34u']\n",
    "    return set(sw.words('english') + custom_stop_words)\n",
    "\n",
    "# collapse word inflections into single representation\n",
    "def lemmatize(token, tag):\n",
    "    tag = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "# tokenize corpus\n",
    "def cab_tokenizer(document):\n",
    "    tokens = []\n",
    "    sw = define_sw()\n",
    "\n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # tokenize each sentence\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in sw:\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "\n",
    "            # Append lemmatized token to list\n",
    "            tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "print(\"Load Stemming, Lemmatization, Tokenization Utilty Functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.2 s, sys: 281 ms, total: 26.4 s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Vector fitting, bag of words model utilizing count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#slice main df for faster prototyping\n",
    "allTweets = allTweets[0:100]\n",
    "allTweets = allTweets.reindex(columns=['Tweet'])\n",
    "\n",
    "#generate vectorizer\n",
    "def gen_vector():\n",
    "    return CountVectorizer(tokenizer=cab_tokenizer,ngram_range=(1,2),\n",
    "                                   min_df=0.15, max_df=0.85)\n",
    "\n",
    "#fit count vectoizer to the supplied corpus, return term frequency matrix/feature names\n",
    "def vectorize(tf_vectorizer, df):\n",
    "    tf_matrix = tf_vectorizer.fit_transform(allTweets['Tweet'])\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    return tf_matrix, tf_feature_names\n",
    "\n",
    "#generate and apply count vector to corpus\n",
    "cv = gen_vector()\n",
    "tf_matrix, tf_feature_names = vectorize(cv, allTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Topic Distribution generation\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def create_lda(num_topics,tf_matrix):\n",
    "    return LatentDirichletAllocation(n_components=num_topics,max_iter=5,\n",
    "                                     learning_method='online',learning_offset=50,\n",
    "                                     random_state=0).fit(tf_matrix)\n",
    "\n",
    "#return normalized topic-word distribution\n",
    "def create_tw_dist(model):\n",
    "    normTWDist = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    #check for validity..\n",
    "    print(\"Topic Word Dist Overview\")\n",
    "    print(normTWDist.shape)\n",
    "    print(normTWDist)\n",
    "    print(sum(normTWDist[0]))\n",
    "    \n",
    "    return normTWDist\n",
    "\n",
    "#return normalized document-topic distribution\n",
    "def create_dt_dist(model):\n",
    "    normDTDist = lda.transform(tf)\n",
    "    \n",
    "    print(\"\\nDocument Topic Dist Overview\")\n",
    "    print(normDTDist.shape)\n",
    "    print(normDTDist[0].sum())\n",
    "    print(normDTDist)\n",
    "    \n",
    "    return normDTDist\n",
    "\n",
    "#fit LDA model using count vector, retrieve distribtuions\n",
    "lda = create_lda(5, tf_matrix)\n",
    "norm_tw = create_tw_dist(lda)\n",
    "norm_dt = create_dt_dist(lda)\n",
    "\n",
    "print(\"Models fitted, distributions retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate topic distribution entropy for each document (user)\n",
    "import scipy as scipy\n",
    "\n",
    "#calculate entropy for a given sequence of values\n",
    "def returnEnt(x):\n",
    "    return scipy.stats.entropy(x)\n",
    "\n",
    "#apply function across entire axis (all row entry entropy calculated)\n",
    "allEnt = np.apply_along_axis(returnEnt, axis=1, arr=norm_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10974236  0.09999156  0.71152595  0.02700171 -0.22358714]\n",
      " [ 0.58049525  0.5740843   0.34687809 -0.87238109  0.89442719]\n",
      " [-0.40004423 -0.40429824 -0.35285085  0.28184004 -0.22362552]\n",
      " [ 0.3286334   0.34499756 -0.35263383  0.28163541 -0.22356904]\n",
      " [-0.61882678 -0.61477518 -0.35291936  0.28190394 -0.22364549]]\n",
      "[[-0.29616453 -0.29616467  0.03063414  0.85785877 -0.29616371]\n",
      " [-0.26920132 -0.2692018  -0.07394533 -0.26919862  0.88154707]\n",
      " [-0.22360785 -0.22360702 -0.22360693  0.89442719 -0.22360539]\n",
      " [-0.22360788 -0.22360648 -0.22360617  0.89442719 -0.22360666]\n",
      " [-0.22360725 -0.22360562 -0.22360734  0.89442719 -0.22360698]]\n"
     ]
    }
   ],
   "source": [
    "#calculate GOSS/LOSS values for each document (user)\n",
    "from math import sqrt\n",
    "\n",
    "sampleDT = norm_dt[:5, :] #portion for prototyping\n",
    "\n",
    "#calculate GOSS score for a particular user/topic (i/k) combination\n",
    "def GOSS(topicDist,i,k):\n",
    "    \n",
    "    #1.0 return mu(xk) for specific topic, sum topic probabilities for all users, average across all users\n",
    "    muxk = np.sum(topicDist[:,k]) / topicDist.shape[0]\n",
    "    \n",
    "    #2.0 calculate muXK diff - GOSS equation numerator\n",
    "    muxkDiff = topicDist[i,k] - muxk\n",
    "    \n",
    "    #3.0 for all users specific topic probability, sum the squared difference of\n",
    "    #their relevant topic probability, find the square of this sum\n",
    "    gossLower = 0\n",
    "    for userProb in topicDist[:,k]:\n",
    "        gossLower += (userProb - muxk) ** 2\n",
    "    \n",
    "    #3.1 find sqrt of gossLower\n",
    "    gossLower = sqrt(gossLower)\n",
    "    \n",
    "    #4.0 divide gossUpper by gossLower to find final GOSS score for particular topic\n",
    "    return muxkDiff / gossLower\n",
    "    \n",
    "#calculate GOSS scores for all users\n",
    "def allGOSS(topicDist):  \n",
    "    allGOSS=[]\n",
    "    for user in range(topicDist.shape[0]): #each user\n",
    "        tempGOSS = list(GOSS(topicDist,user,topic) for topic in range(topicDist.shape[1])) #calculate all GOSS scores per topic\n",
    "        allGOSS.append(tempGOSS) #store all GOSS via nested lists\n",
    "\n",
    "    return np.array(allGOSS) #return np array\n",
    "\n",
    "#calculate LOSS score for a particular user/topic (i/k) combination\n",
    "def LOSS(topicDist,i,k):\n",
    "    #1.0 return mu(xi) for specific user, sum topic probabilities, return average\n",
    "    muxi = np.sum(topicDist[i,:]) / topicDist.shape[1]\n",
    "    \n",
    "    #2.0 calculate muXI diff - GOSS equation numerator\n",
    "    muxiDiff = topicDist[i,k] - muxi\n",
    "    \n",
    "    #3.0 for all topics (k) and a specific user (i), sum the squared difference of\n",
    "    #all associated topic probabilities and mu(xi), find the square of this sum\n",
    "    lossLower = 0\n",
    "    for userProb in topicDist[i,:]:\n",
    "        lossLower += (userProb - muxi) ** 2\n",
    "    \n",
    "    #3.1 find sqrt of gossLower\n",
    "    lossLower = sqrt(lossLower)\n",
    "    \n",
    "    #4.0 divide gossUpper by gossLower to find final GOSS score for particular topic\n",
    "    return muxiDiff / lossLower\n",
    "    \n",
    "#calculate GOSS scores for all users\n",
    "def allLOSS(topicDist):  \n",
    "    allLOSS=[]\n",
    "    \n",
    "    for user in range(topicDist.shape[0]): #each user\n",
    "        tempLOSS = list(LOSS(topicDist,user,topic) for topic in range(topicDist.shape[1])) #calculate all LOSS scores per topic\n",
    "        allLOSS.append(tempLOSS) #store all LOSS scores for each user via nested lists\n",
    "    return np.array(allLOSS) #return np array\n",
    "\n",
    "print(allGOSS(sampleDT))\n",
    "print(allLOSS(sampleDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173 172 183 188  76 146 189 218 171 238  53 180 218 225 195 226  23 261\n",
      " 237 172 179  69 242 202 185 141 183 180 138 203  32 175 174 229 107 236\n",
      " 189 166 213 237 145 187  32 178 124 152 151 117 238 149 168 123 193 180\n",
      " 231   6  20  84 174 210  92  66 212 187 172 173 210 201 152 140 217 119\n",
      " 238 167  63   0 187 114 154 197 113 218 190  70 231 165 245  65 205 119\n",
      " 150 135 153 210 217 204 197 191  11 195]\n",
      "(100,)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "#pre-classification clustering/feature generation\n",
    "import pandas as pd\n",
    "import scipy as scipy\n",
    "\n",
    "# #convert sparse matrix to pandas df for intuition\n",
    "# pdTD = pd.DataFrame(tf.toarray(), columns=tf_vectorizer.get_feature_names())\n",
    "# pdTD.head(5)\n",
    "\n",
    "#calculate entropy for a given sequence of values\n",
    "def returnEnt(x):\n",
    "    return scipy.stats.entropy(x)\n",
    "\n",
    "#retrieve entropy across all row entries\n",
    "def appendEnt(TDVec):\n",
    "    return np.apply_along_axis(returnEnt, axis=1, arr=TDVec)\n",
    "\n",
    "#calculate unique values for document\n",
    "def returnUni(x):\n",
    "    return np.count_nonzero(x==1)\n",
    "\n",
    "#retrieve for all docs, append to feature matrix\n",
    "def appendUni(featDF,TDVec):\n",
    "    tempUni = np.apply_along_axis(returnUni, axis=1, arr=TDVec)\n",
    "    \n",
    "x = np.array([[1,2,3,1,1],\n",
    "              [1,2,3,1,1]])\n",
    "\n",
    "#apply function across entire axis (all row entry entropy calculated)\n",
    "# allEnt = np.apply_along_axis(returnEnt, axis=1, arr=normDTDist)\n",
    "a = np.apply_along_axis(returnUni, 1, scipy.sparse.csr_matrix.todense(tf)) #currently using dense representation.. may run into issues\n",
    "print(a)\n",
    "print(type(tf))\n",
    "# b = np.apply_along_axis(returnEnt, axis=1, arr=tf)\n",
    "\n",
    "# r = np.apply_along_axis(returnUni,axis=1, arr=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['feature_engineering', 'initial_clustering', 'final_classification'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('../app/input_parameters.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "toDo\n",
    "Feature Engineering\n",
    "Unique (singly occurring) words\n",
    "Kmeans clustering\n",
    "\n",
    "\n",
    "Hyperlink counts\n",
    "Num unique words\n",
    "Collapse features into clustering/classification dataframes\n",
    "\n",
    "LaterDo\n",
    "Misc\n",
    "14 million SQL -> csv conversion, honeypot styling\n",
    "Test LOSS/GOSS -> clarify functionality\n",
    "\n",
    "Feature engineering\n",
    "Number frequent words -> define \"frequent\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
